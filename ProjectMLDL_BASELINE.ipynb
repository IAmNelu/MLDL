{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectMLDL_BASELINE_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fd4f6064d5c4ab5a1c8b6b001767169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6346547a6bc04bfd846f79d6836a960e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eab0364d2c554e2289d61fa8673c02fb",
              "IPY_MODEL_b2bcd6eac33e4e748d8cd323cbb87236"
            ]
          }
        },
        "6346547a6bc04bfd846f79d6836a960e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eab0364d2c554e2289d61fa8673c02fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_19c5a75e7573427f9db7b3d3a48eb74a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_925699e651984b24a0d2f84a2f3fc318"
          }
        },
        "b2bcd6eac33e4e748d8cd323cbb87236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8e6a837d1f2e4d33829383a8e006f8f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 76.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b08f8da3e04449b1abd621ba0a35def2"
          }
        },
        "19c5a75e7573427f9db7b3d3a48eb74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "925699e651984b24a0d2f84a2f3fc318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e6a837d1f2e4d33829383a8e006f8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b08f8da3e04449b1abd621ba0a35def2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAmNelu/MLDL/blob/master/ProjectMLDL_BASELINE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFjPFtlrK0Y",
        "colab_type": "text"
      },
      "source": [
        "# Domain Adaptation Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC1sqv5BrG3R",
        "colab_type": "text"
      },
      "source": [
        "##Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGUUe-iZrQCQ",
        "colab_type": "text"
      },
      "source": [
        "Run Mount Drive then Copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzclAdrgr-Bn",
        "colab_type": "text"
      },
      "source": [
        "Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9OwuPWSlt-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62iMqhIUrZJv",
        "colab_type": "text"
      },
      "source": [
        "Copy 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a4vHHzI8Kti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip /content/drive/My\\ Drive/synROD.zip -d /content/\n",
        "# !unzip /content/drive/My\\ Drive/ROD.zip -d /content/\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1U0WobVS9A36UVEMcEK4YISHRqeDvFW78' -O rod-split_sync.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V45I_smbkFy5",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1QtntN8kou6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.models import resnet18\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Function\n",
        "from torchvision import transforms\n",
        "from torch.backends import cudnn\n",
        "import torch.nn.functional as Fu\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import numbers\n",
        "import logging\n",
        "import os.path\n",
        "import random\n",
        "import torch\n",
        "import copy\n",
        "import time\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axkn6Lm7sgl6",
        "colab_type": "text"
      },
      "source": [
        "##Functions & Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txZDH-EJkM7d",
        "colab_type": "text"
      },
      "source": [
        "###Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqbrnEbn6oHY",
        "colab_type": "text"
      },
      "source": [
        "####Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCBaAFs-nI_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pil_loader(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    img = Image.open(f)\n",
        "    return img.convert('RGB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj736EeK6eYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_image(image, t1, t2, t3, transform):\n",
        "\n",
        "  rotation = random.randint(0,3)\n",
        "  if rotation == 0:\n",
        "    image = transform(image)\n",
        "    return image, rotation\n",
        "\n",
        "  elif rotation == 1:\n",
        "    image_rotate = t3(image)\n",
        "    image_rotate = transform(image_rotate)\n",
        "    return image_rotate, rotation\n",
        "\n",
        "  elif rotation == 2:\n",
        "    image_rotate = t2(image)\n",
        "    image_rotate = transform(image_rotate)\n",
        "    return image_rotate, rotation\n",
        "\n",
        "  elif rotation == 3:\n",
        "    image_rotate = t1(image)\n",
        "    image_rotate = transform(image_rotate)\n",
        "    return image_rotate, rotation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6lf7Fe6NfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_dictionary(path_file, ROD=False):\n",
        "  dictionary = {}\n",
        "  reverse_dictionary = {}\n",
        "  f = open(path_file, \"r\")\n",
        "  \n",
        "  for line in f:\n",
        "    path, label_index = line.split(\" \")\n",
        "    if ROD == False:\n",
        "      label_name = path.split(\"/\")[0]\n",
        "    else:\n",
        "      label_name = path.split(\"/\")[1]\n",
        "\n",
        "    dictionary[int(label_index)] = label_name\n",
        "    reverse_dictionary[label_name] = int(label_index)\n",
        "\n",
        "  return dictionary, reverse_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGI80zrkPvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_set(dataset, size=30000): #modify\n",
        "\n",
        "  dictionary = dataset.get_samples_label()\n",
        "  X = list(dictionary.keys())\n",
        "  y = list(dictionary.values())\n",
        "\n",
        "  indexes, _, _, _ = train_test_split(X, y, stratify=y, train_size=size)\n",
        "\n",
        "  return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ErRYQZ6xJh",
        "colab_type": "text"
      },
      "source": [
        "####ROD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oXZOXG2kPoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ROD(VisionDataset):\n",
        "  def __init__(self, root=None, file_path=None, validation=False, pretext=False, transform=None, target_transform=None):\n",
        "    super(ROD, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "    if file_path == None:\n",
        "      print(\"miss the path of the file!\")\n",
        "\n",
        "    if root == None:\n",
        "      print(\"miss the path of the dataset!\")\n",
        "      \n",
        "    #rotation\n",
        "    self.t1 = transforms.RandomRotation(degrees=[90, 90])\n",
        "    self.t2 = transforms.RandomRotation(degrees=[180, 180])\n",
        "    self.t3 = transforms.RandomRotation(degrees=[270, 270])\n",
        "  \n",
        "    self.samples_rgb = {}    #dictionary image_index -> path of the image\n",
        "    self.samples_depth = {}  #dictionary image_index -> path of the image\n",
        "    self.samples_label = {}  #dictionary image_index -> index of the label\n",
        "    self.labels_index = {}   #dictionary label_index -> name of the label\n",
        "    self.labels_name = {}    #dictionary label_names -> index of the label\n",
        "\n",
        "    self.labels_index, self.labels_name = initialize_dictionary(file_path, ROD=True) \n",
        "    self.pretext = pretext\n",
        "    self.validation = validation\n",
        "\n",
        "\n",
        "    # Read files and load images in memory  \n",
        "    idx = 0       #actual number of image\n",
        "    f = open(file_path, \"r\")\n",
        "    for line in f:\n",
        "      path, label = line.split(\" \")\n",
        "\n",
        "      rgb_path = path.replace(\"???\",\"rgb\",1)\n",
        "      rgb_path = rgb_path.replace(\"***\",\"crop\",1)\n",
        "      self.samples_rgb[idx] = pil_loader(root+\"/\"+rgb_path)\n",
        "\n",
        "      depth_path = path.replace(\"???\",\"surfnorm\",1)\n",
        "      depth_path = depth_path.replace(\"***\",\"depthcrop\",1)\n",
        "      self.samples_depth[idx] = pil_loader(root+\"/\"+depth_path)\n",
        "\n",
        "      self.samples_label[idx] = int(label)\n",
        "      idx += 1\n",
        "\n",
        "   \n",
        "  def __getitem__(self, index):\n",
        "    image_rgb = self.samples_rgb.get(index)\n",
        "    image_depth = self.samples_depth.get(index)\n",
        "\n",
        "    if self.validation: # if validation is true, target transformations are applied\n",
        "      image_rgb = self.target_transform(image_rgb)\n",
        "      image_depth = self.target_transform(image_depth)\n",
        "\n",
        "      label = self.samples_label.get(index)\n",
        "      return [image_rgb, image_depth], label\n",
        "\n",
        "    image_rgb = self.transform[0](image_rgb) #resize\n",
        "    image_depth = self.transform[0](image_depth)\n",
        "\n",
        "    image_rgb, image_depth = self.transform[1][0](image_rgb, image_depth) # Random Crop\n",
        "    image_rgb, image_depth = self.transform[1][1](image_rgb, image_depth) # Horizontal flip\n",
        "\n",
        "    if not self.pretext: \n",
        "      image_rgb = self.transform[2](image_rgb) # To Tensor & Normlize \n",
        "      image_depth = self.transform[2](image_depth)\n",
        "\n",
        "      label = self.samples_label.get(index)\n",
        "      \n",
        "      return [image_rgb, image_depth], label\n",
        "\n",
        "    else: # rotate the image\n",
        "      image_rgb, rotation_rgb = rotate_image(image_rgb, self.t1, self.t2, self.t3, self.transform[2]) #Rotation RGB\n",
        "      image_depth, rotation_depth = rotate_image(image_depth, self.t1, self.t2, self.t3, self.transform[2]) #Rotation Depth\n",
        "      \n",
        "      relative_rotation =  abs(rotation_rgb - rotation_depth)%4  # encoding relative rotation\n",
        "\n",
        "      return [image_rgb, image_depth], relative_rotation\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    length = len(self.samples_rgb)\n",
        "    return length \n",
        "\n",
        "  def get_labels_dictionary(self):\n",
        "    return self.labels_index, self.labels_name\n",
        "\n",
        "  def get_samples_label(self):\n",
        "    return self.samples_label\n",
        "\n",
        "  def set_pretext(self, pretext):\n",
        "    self.pretext = pretext\n",
        "\n",
        "  def set_validation(self, validation):\n",
        "    self.validation = validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyOflxp96zhZ",
        "colab_type": "text"
      },
      "source": [
        "####synROD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn7MCiZJ63zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class synROD(VisionDataset): \n",
        "  def __init__(self, root=None, file_path=None, pretext=False, transform=None, target_transform=None):\n",
        "    super(synROD, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "    if file_path == None:\n",
        "      print(\"miss the path of the file!\")\n",
        "\n",
        "    if root == None:\n",
        "      print(\"miss the path of the dataset!\")\n",
        "    \n",
        "    #rotation\n",
        "    self.t1 = transforms.RandomRotation(degrees=[90, 90])\n",
        "    self.t2 = transforms.RandomRotation(degrees=[180, 180])\n",
        "    self.t3 = transforms.RandomRotation(degrees=[270, 270])\n",
        "  \n",
        "    self.samples_rgb = {}    #dictionary image_index -> path of the image\n",
        "    self.samples_depth = {}  #dictionary image_index -> path of the image\n",
        "    self.samples_label = {}  #dictionary image_index -> index of the label\n",
        "    self.labels_index = {}   #dictionary label_index -> name of the label\n",
        "    self.labels_name = {}    #dictionary label_names -> index of the label\n",
        "\n",
        "    self.labels_index, self.labels_name = initialize_dictionary(file_path)\n",
        " \n",
        "    self.pretext = pretext\n",
        "    \n",
        "    # Read files and load path in memory  \n",
        "    idx = 0                  #actual number of images   \n",
        "    f = open(file_path, \"r\")\n",
        "\n",
        "    for line in f:\n",
        "      path, label = line.split(\" \")\n",
        "      \n",
        "      rgb_path = path.replace(\"***\",\"rgb\",1)\n",
        "      self.samples_rgb[idx] = root+\"/\"+rgb_path\n",
        "      \n",
        "      depth_path = path.replace(\"***\",\"depth\",1)\n",
        "      self.samples_depth[idx] = root+\"/\"+depth_path\n",
        "\n",
        "      self.samples_label[idx] = int(label)\n",
        "      idx += 1\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    path_rgb = self.samples_rgb.get(index)\n",
        "    image_rgb = pil_loader(path_rgb)          #load image\n",
        "    image_rgb = self.transform[0](image_rgb)  #resize imahe\n",
        "\n",
        "    path_depth = self.samples_depth.get(index)\n",
        "    image_depth = pil_loader(path_depth)\n",
        "    image_depth = self.transform[0](image_depth)\n",
        "\n",
        "    image_rgb, image_depth = self.transform[1][0](image_rgb, image_depth) # Random Crop\n",
        "    image_rgb, image_depth = self.transform[1][1](image_rgb, image_depth) # Horizontal flip\n",
        "\n",
        "    if not self.pretext:\n",
        "      image_rgb = self.transform[2](image_rgb)\n",
        "      image_depth = self.transform[2](image_depth)\n",
        "\n",
        "      label = self.samples_label.get(index)\n",
        "      \n",
        "      return [image_rgb, image_depth], label\n",
        "\n",
        "    else:\n",
        "      image_rgb, rotation_rgb = rotate_image(image_rgb, self.t1, self.t2, self.t3, self.transform[2])\n",
        "      image_depth, rotation_depth = rotate_image(image_depth, self.t1, self.t2, self.t3, self.transform[2])\n",
        "\n",
        "      relative_rotation = abs(rotation_rgb - rotation_depth) % 4  # encoding relative rotation\n",
        "\n",
        "      return [image_rgb, image_depth], relative_rotation\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    length = len(self.samples_rgb)\n",
        "    return length \n",
        "\n",
        "  def get_labels_dictionary(self):\n",
        "    return self.labels_index, self.labels_name\n",
        "\n",
        "  def get_samples_label(self):\n",
        "    return self.samples_label\n",
        "\n",
        "  def set_pretext(self, pretext):\n",
        "    self.pretext = pretext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5trOPX1TtDSH",
        "colab_type": "text"
      },
      "source": [
        "### Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgi_YjTuw8hy",
        "colab_type": "text"
      },
      "source": [
        "####Helping Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aLqenSww8Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "  \"\"\"3x3 convolution with padding\"\"\"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                   padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "  \"\"\"1x1 convolution\"\"\"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "               base_width=64, dilation=1, norm_layer=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    if groups != 1 or base_width != 64:\n",
        "      raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "    if dilation > 1:\n",
        "      raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "    # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = norm_layer(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = norm_layer(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK6Y0PKJ408U",
        "colab_type": "text"
      },
      "source": [
        "####MagicNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCNPfRV29VC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MagicNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, complete=True, num_classes=1000, zero_init_residual=False,\n",
        "                groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                norm_layer=None):\n",
        "    super(MagicNet, self).__init__()\n",
        "    if norm_layer is None:\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "    self._norm_layer = norm_layer\n",
        "\n",
        "    self.complete = complete\n",
        "    self.inplanes = 64\n",
        "    self.dilation = 1\n",
        "    if replace_stride_with_dilation is None:\n",
        "        # each element in the tuple indicates if we should replace\n",
        "        # the 2x2 stride with a dilated convolution instead\n",
        "        replace_stride_with_dilation = [False, False, False]\n",
        "    if len(replace_stride_with_dilation) != 3:\n",
        "        raise ValueError(\"replace_stride_with_dilation should be None or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "    self.groups = groups\n",
        "    self.base_width = width_per_group\n",
        "    # CODA Ec\n",
        "    # First layer 7x7, 64 neurons stride 2\n",
        "    self.conv1_c = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1_c = norm_layer(self.inplanes)\n",
        "    self.relu_c = nn.ReLU(inplace=True)\n",
        "    self.maxpool_c = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.layer1_c = self._make_layer(block, 64, layers[0]) \n",
        "    self.layer2_c = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) \n",
        "    self.layer3_c = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "    self.layer4_c = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "    self.inplanes = 64  # reset inplanes number\n",
        "    # CODA Ed\n",
        "    # First layer 7x7, 64 neurons stride 2\n",
        "    self.conv1_d = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1_d = norm_layer(self.inplanes)\n",
        "    self.relu_d = nn.ReLU(inplace=True)\n",
        "    self.maxpool_d = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.layer1_d = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2_d = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])  \n",
        "    self.layer3_d = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) \n",
        "    self.layer4_d = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) \n",
        "\n",
        "    multiplier = 1\n",
        "    if  complete:\n",
        "      multiplier = 2\n",
        "    \n",
        "    # Head M\n",
        "    self.main_head = nn.Sequential(OrderedDict([\n",
        "        ('avgpool_m',   nn.AdaptiveAvgPool2d((1, 1))), # https://discuss.pytorch.org/t/adaptive-avg-pool2d-vs-avg-pool2d/27011\n",
        "        ('flatten',     nn.Flatten(1)),\n",
        "        ('fc_m1000',    nn.Linear(512 * block.expansion * multiplier, 1000)),# multiplied by 2: 512 for RGB and 512 for depth\n",
        "        ('normal_m',    nn.BatchNorm1d(1000)),\n",
        "        ('relu_mfc',    nn.ReLU(inplace=True)),\n",
        "        ('drop_m1000',  nn.Dropout()),\n",
        "        ('fc_mC',       nn.Linear(1000, num_classes)),\n",
        "    ]))\n",
        "\n",
        "\n",
        "    # Head P\n",
        "    self.pretext_head = nn.Sequential(OrderedDict([\n",
        "        ('conv1_p',     nn.Conv2d(512*multiplier, 100, kernel_size=1,stride=1)),\n",
        "        ('normal_pcv1', nn.BatchNorm2d(100)),\n",
        "        ('relu_pc1',    nn.ReLU(inplace=True)),\n",
        "        ('conv2_p',     nn.Conv2d(100, 100, kernel_size=3,stride=2)),\n",
        "        ('normal_pcv2', nn.BatchNorm2d(100)),\n",
        "        ('relu_pc2',    nn.ReLU(inplace=True)),\n",
        "        ('flatten',     nn.Flatten(1)),\n",
        "        ('fc_p100',     nn.Linear(900, 100)),\n",
        "        ('normal_pfc',  nn.BatchNorm1d(100)),\n",
        "        ('relu_pfc',    nn.ReLU(inplace=True)),\n",
        "        ('drop_p100',   nn.Dropout()),\n",
        "        ('fc_p4',       nn.Linear(100, 4)),\n",
        "    ]))\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "    norm_layer = self._norm_layer\n",
        "    downsample = None\n",
        "    previous_dilation = self.dilation\n",
        "    if dilate:\n",
        "      self.dilation *= stride\n",
        "      stride = 1\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "          norm_layer(planes * block.expansion),\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                        self.base_width, previous_dilation, norm_layer))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                          base_width=self.base_width, dilation=self.dilation,\n",
        "                          norm_layer=norm_layer))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def _forward_impl(self, rgb_x=None, dp_x=None, main_task=True):\n",
        "    \n",
        "    rgb_x = self.conv1_c(rgb_x)\n",
        "    rgb_x = self.bn1_c(rgb_x)\n",
        "    rgb_x = self.relu_c(rgb_x)\n",
        "    rgb_x = self.maxpool_c(rgb_x)\n",
        "\n",
        "    rgb_x = self.layer1_c(rgb_x)\n",
        "    rgb_x = self.layer2_c(rgb_x)\n",
        "    rgb_x = self.layer3_c(rgb_x)\n",
        "    rgb_x = self.layer4_c(rgb_x)\n",
        "\n",
        "\n",
        "    dp_x = self.conv1_d(dp_x)\n",
        "    dp_x = self.bn1_d(dp_x)\n",
        "    dp_x = self.relu_d(dp_x)\n",
        "    dp_x = self.maxpool_d(dp_x)\n",
        "\n",
        "    dp_x = self.layer1_d(dp_x)\n",
        "    dp_x = self.layer2_d(dp_x)\n",
        "    dp_x = self.layer3_d(dp_x)\n",
        "    dp_x = self.layer4_d(dp_x)\n",
        "\n",
        "    d_rgb_x = torch.cat((dp_x, rgb_x), 1)  #concatenation\n",
        "\n",
        "    if main_task:\n",
        "      d_rgb_x = self.main_head(d_rgb_x)\n",
        "    else:\n",
        "      d_rgb_x = self.pretext_head(d_rgb_x)\n",
        "    return d_rgb_x\n",
        "\n",
        "  def forward(self, rgb_x=None, dp_x=None, main_task=True):\n",
        "    return self._forward_impl(rgb_x=rgb_x, dp_x=dp_x, main_task=main_task)\n",
        "\n",
        "  def initNet(self):\n",
        "    netSource = resnet18(pretrained=True)\n",
        "    # copy Ec\n",
        "    self.conv1_c.weight.data = copy.deepcopy(netSource.conv1.weight.data)\n",
        "    self.bn1_c.weight.data = copy.deepcopy(netSource.bn1.weight.data)\n",
        "    self.bn1_c.bias.data = copy.deepcopy(netSource.bn1.bias.data)\n",
        "    self.bn1_c.running_mean.data = copy.deepcopy(netSource.bn1.running_mean.data)\n",
        "    self.bn1_c.running_var.data = copy.deepcopy(netSource.bn1.running_var.data)\n",
        "    self.layer1_c = copy.deepcopy(netSource.layer1)\n",
        "    self.layer2_c = copy.deepcopy(netSource.layer2)\n",
        "    self.layer3_c = copy.deepcopy(netSource.layer3)\n",
        "    self.layer4_c = copy.deepcopy(netSource.layer4)\n",
        "    # copy Ed\n",
        "    self.conv1_d.weight.data = copy.deepcopy(netSource.conv1.weight.data)\n",
        "    self.bn1_d.weight.data = copy.deepcopy(netSource.bn1.weight.data)\n",
        "    self.bn1_d.bias.data = copy.deepcopy(netSource.bn1.bias.data)\n",
        "    self.bn1_d.running_mean.data = copy.deepcopy(netSource.bn1.running_mean.data)\n",
        "    self.bn1_d.running_var.data = copy.deepcopy(netSource.bn1.running_var.data)\n",
        "    self.layer1_d = copy.deepcopy(netSource.layer1)\n",
        "    self.layer2_d = copy.deepcopy(netSource.layer2)\n",
        "    self.layer3_d = copy.deepcopy(netSource.layer3)\n",
        "    self.layer4_d = copy.deepcopy(netSource.layer4)\n",
        "\n",
        "    for module in self.main_head:\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        if not module.bias.data is None:\n",
        "            module.bias.data.zero_()\n",
        "    for module in self.pretext_head:\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        if not module.bias.data is None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "  def load_state_from_file(self, file_name):\n",
        "    if os.path.isfile(file_name):\n",
        "      self.load_state_dict(torch.load(file_name))\n",
        "      print(f'Model {file_name} loaded succesfully')\n",
        "    else:\n",
        "      raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file_name)\n",
        "\n",
        "  def save_state_on_file(self, file_name):\n",
        "    torch.save(self.state_dict(), file_name)\n",
        "    print(f'Model saved succesfully on file {file_name}')\n",
        "      \n",
        "def _magicnet(block, pretrained, complete, **kwargs):\n",
        "  model = MagicNet(block, [2, 2, 2, 2], complete=complete, **kwargs)\n",
        "  if pretrained:\n",
        "    if os.path.isfile('magic_net.pth'):\n",
        "      model.load_state_dict(torch.load(\"magic_net.pth\"))\n",
        "    else:\n",
        "      model.initNet()\n",
        "      torch.save(model.state_dict(), 'magic_net.pth')\n",
        "  return model\n",
        "\n",
        "\n",
        "def magicnet(pretrained=False, complete=True, **kwargs): # Complete uses both rgb and depth\n",
        "  return _magicnet(BasicBlock, pretrained, complete=complete, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfla0S2U1jZn",
        "colab_type": "text"
      },
      "source": [
        "###Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVJiaVTTlC9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy_loss(logits):\n",
        "    p_softmax = Fu.softmax(logits, dim=1)\n",
        "    mask = p_softmax.ge(0.000001)  # greater or equal to, used for numerical stability\n",
        "    mask_out = torch.masked_select(p_softmax, mask)\n",
        "    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n",
        "    return entropy / float(p_softmax.size(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTD1wCeyyBAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTimes(start_s, end_s, for_what='test'):\n",
        "  time_taken = end_s - start_s\n",
        "  seconds = int(time_taken%60)\n",
        "  time_taken -= seconds\n",
        "  minutes = int(time_taken/60)\n",
        "  ores = int(minutes/60)\n",
        "  minutes %= 60\n",
        "  minutes = int(minutes)\n",
        "  ores_s = str(ores)\n",
        "  minutes_s = str(minutes)\n",
        "  seconds_s = str(seconds)\n",
        "  if ores < 10:\n",
        "    ores_s = '0' + ores_s\n",
        "  if minutes < 10:\n",
        "    minutes_s = '0' + minutes_s\n",
        "  if seconds < 10:\n",
        "    seconds_s = '0' + seconds_s\n",
        "  print(f'Time taken for {for_what}: {ores_s}:{minutes_s}:{seconds_s}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5K39q3MaWlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_losses(losses, sampling_size=10, labels=['Main Task Loss', 'synROD Pretext Task', 'ROD Pretext Task'], \n",
        "                colors=['#7C55BD','#42BD5B','#BD7C2F'], fig_size=(10,7), \n",
        "                        legend_pos=(0.74,0.99), save_to_png=False, title='Loss', x_label='Step #', y_label='Loss'):\n",
        "  x = np.arange(len(losses[0]))\n",
        "  fig = plt.figure(figsize=fig_size)\n",
        "  ax = fig.add_subplot(111)\n",
        "  for i, y in enumerate(losses):\n",
        "    if len(y) > 0:\n",
        "      ax.plot(x[::sampling_size], y[::sampling_size], linestyle='-',  marker=\"h\", c=colors[i], label=labels[i])\n",
        "  ax.plot(x, [min(losses[0])]*len(losses[0]), c='r', linestyle='--',  marker=\"\", label=f'Min Loss {min(losses[0]):.2f}')\n",
        "  plt.legend(bbox_to_anchor=legend_pos, loc='upper left', borderaxespad=0.)\n",
        "  \n",
        "  plt.grid(color='w')\n",
        "  # plt.ylim(0)\n",
        "  ax.set_axisbelow(True)\n",
        "  ax.set_facecolor('#EAEAF2')\n",
        "  plt.xlabel(x_label)\n",
        "  plt.ylabel(y_label)\n",
        "  ax.yaxis.grid('Ture')\n",
        "  plt.title(title, fontsize=25)\n",
        "  if save_to_png:\n",
        "    plt.savefig(f\"{title}.png\", dpi=240)\n",
        "  else:\n",
        "    plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFxKQCVx4_Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_accuracies(accuracies, test_accuracy=None, sampling_size=1, \n",
        "                  labels=['Validation Accuracies', 'Test Accuracy'], \n",
        "                  colors=['#7C55BD','#42BD5B'], fig_size=(10,7), \n",
        "                          legend_pos=(0.7,0.17), save_to_png=False, \n",
        "                          title='Accuracies in %', x_label='Epochs', y_label='Accuracy'):\n",
        "  x = np.arange(len(accuracies))\n",
        "  fig = plt.figure(figsize=fig_size)\n",
        "  ax = fig.add_subplot(111)\n",
        "\n",
        "  ax.plot(x[::sampling_size], accuracies[::sampling_size], linestyle='-',  marker=\"h\", c=colors[0], label=labels[0])\n",
        "  if not test_accuracy is None:\n",
        "    ax.plot(x, [test_accuracy]*len(accuracies), c=colors[1], linestyle='-',  marker=\"\", label=labels[1])\n",
        "  ax.plot(x, [max(accuracies)]*len(accuracies), c='r', linestyle='--',  marker=\"\", label=f'Max Accuracy {max(accuracies):.2f}%')\n",
        "  plt.legend(bbox_to_anchor=legend_pos, loc='upper left', borderaxespad=0.)\n",
        "  \n",
        "  plt.grid(color='w')\n",
        "  # plt.ylim(0)\n",
        "  ax.set_axisbelow(True)\n",
        "  ax.set_facecolor('#EAEAF2')\n",
        "  plt.xlabel(x_label)\n",
        "  plt.ylabel(y_label)\n",
        "  ax.yaxis.grid('Ture')\n",
        "  plt.title(title, fontsize=25)\n",
        "  if save_to_png:\n",
        "    plt.savefig(f\"{title}.png\", dpi=240)\n",
        "  else:\n",
        "    plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igZAtIELgbaz",
        "colab_type": "text"
      },
      "source": [
        "###Validation / test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QxB7JhigaGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Validation\n",
        "def test_validate(net, valid_dataloader, len_data, DEVICE='cuda', padding_text='A'):\n",
        "  net = net.to(DEVICE)\n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "  running_corrects = 0\n",
        "  for images, labels in tqdm(valid_dataloader):\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    #concatenate RGB and Depth for Syn\n",
        "    imagesRGB = images[0].to(DEVICE)\n",
        "    imagesD = images[1].to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = net(rgb_x=imagesRGB, dp_x=imagesD)  \n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  accuracy = running_corrects / float(len_data)\n",
        "\n",
        "  print(f'{padding_text}ccuracy: {accuracy*100:.2f}%')\n",
        "  net.train(True)\n",
        "  return accuracy * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY19PYsLxzvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utility function to stop the training process of the network\n",
        "def stop_trining_4(array_of_validations):\n",
        "  if len(array_of_validations) < 4:\n",
        "    return False\n",
        "  if array_of_validations[-1] < array_of_validations[-2] < array_of_validations[-3] < array_of_validations[-4]:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def stop_trining_3(array_of_validations):\n",
        "  if len(array_of_validations) < 3:\n",
        "    return False\n",
        "  if array_of_validations[-1] < array_of_validations[-2] < array_of_validations[-3]:\n",
        "    return True\n",
        "  return False\n",
        "  \n",
        "def stop_trining_2(array_of_validations):\n",
        "  if len(array_of_validations) < 2:\n",
        "    return False\n",
        "  if array_of_validations[-1] < array_of_validations[-2]:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwJmpWJWv-Xz",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RevLG2iXwAqt",
        "colab_type": "text"
      },
      "source": [
        "###Prepare Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPfEKMXZwIEz",
        "colab_type": "text"
      },
      "source": [
        "#####Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqHuTIHva1aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' \n",
        "NUM_CLASSES = 47     #actual number of classes\n",
        "BATCH_SIZE = 64      \n",
        "LR = 3e-4            # The Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD\n",
        "WEIGHT_DECAY = 0.05\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "LOG_FREQUENCY = 50\n",
        "VALIDATION_SIZE = BATCH_SIZE * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa3ocL7JwLiD",
        "colab_type": "text"
      },
      "source": [
        "####Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XuOirYydBtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomCrop(object):\n",
        "  def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'):\n",
        "    if isinstance(size, numbers.Number):\n",
        "      self.size = (int(size), int(size))\n",
        "    else:\n",
        "      self.size = size\n",
        "    self.padding = padding\n",
        "    self.pad_if_needed = pad_if_needed\n",
        "    self.fill = fill\n",
        "    self.padding_mode = padding_mode\n",
        "\n",
        "  @staticmethod\n",
        "  def get_params(img, output_size):\n",
        "    w, h = img.size\n",
        "    th, tw = output_size\n",
        "    if w == tw and h == th:\n",
        "      return 0, 0, h, w\n",
        "\n",
        "    i = random.randint(0, h - th)\n",
        "    j = random.randint(0, w - tw)\n",
        "    return i, j, th, tw\n",
        "\n",
        "  def __call__(self, img1, img2):\n",
        "    \n",
        "    if self.padding is not None:\n",
        "      img1 = F.pad(img1, self.padding, self.fill, self.padding_mode)\n",
        "      img2 = F.pad(img2, self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "    # pad the width if needed (img1)\n",
        "    if self.pad_if_needed and img1.size[0] < self.size[1]:\n",
        "      img1 = F.pad(img1, (self.size[1] - img1.size[0], 0), self.fill, self.padding_mode)\n",
        "\n",
        "    # pad the height if needed (img 1)\n",
        "    if self.pad_if_needed and img1.size[1] < self.size[0]:\n",
        "      img1 = F.pad(img1, (0, self.size[0] - img1.size[1]), self.fill, self.padding_mode)\n",
        "\n",
        "      # pad the width if needed (img 2)\n",
        "    if self.pad_if_needed and img2.size[0] < self.size[1]:\n",
        "      img2 = F.pad(img2, (self.size[1] - img2.size[0], 0), self.fill, self.padding_mode)\n",
        "\n",
        "    # pad the height if needed (img 2)\n",
        "    if self.pad_if_needed and img2.size[1] < self.size[0]:\n",
        "      img2 = F.pad(img2, (0, self.size[0] - img2.size[1]), self.fill, self.padding_mode)\n",
        "\n",
        "    i, j, h, w = self.get_params(img1, self.size)\n",
        "\n",
        "    return F.crop(img1, i, j, h, w), F.crop(img2, i, j, h, w)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUsbe8DpdB3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomHorizontalFlip(object):\n",
        "  def __init__(self, p=0.5):\n",
        "    self.p = p\n",
        "\n",
        "  def __call__(self, img1, img2):\n",
        "    if random.random() < self.p:\n",
        "      return F.hflip(img1), F.hflip(img2)\n",
        "    return img1, img2\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + '(p={})'.format(self.p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hybTo-p9kP2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train trasformation\n",
        "train_transform_resize = transforms.Resize(256)\n",
        "train_transform_pair_image = [RandomHorizontalFlip(), RandomCrop(224)] \n",
        "train_transform_final = transforms.Compose([transforms.ToTensor(), \n",
        "                                            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "#test trasformation\n",
        "test_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                     transforms.CenterCrop(224),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOTy7k1EwSHE",
        "colab_type": "text"
      },
      "source": [
        "####Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDbI9gfmGicg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIR_synROD =\"synROD\"\n",
        "DIR_ROD = \"ROD\"\n",
        "synROD_train_split = \"/content/synROD/synARID_50k-split_sync_train1.txt\"\n",
        "ROD_split = \"/content/rod-split_sync.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWex8yDo7PNG",
        "colab_type": "code",
        "outputId": "d9c252e4-72de-40f6-fc20-eea01dcd7686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "synROD_train_dataset = synROD(DIR_synROD,transform=[train_transform_resize, train_transform_pair_image,train_transform_final], file_path=synROD_train_split)\n",
        "ROD_dataset = ROD(DIR_ROD,transform=[train_transform_resize, train_transform_pair_image,train_transform_final], target_transform=test_transform, file_path=ROD_split)\n",
        "\n",
        "print(f'synROD train Dataset: {len(synROD_train_dataset)}')\n",
        "print(f'ROD Dataset: {len(ROD_dataset)}')\n",
        "\n",
        "indexes = reduce_set(synROD_train_dataset, size=len(ROD_dataset))\n",
        "synROD_reduced_dataset = Subset(synROD_train_dataset, indexes)\n",
        "print(f'synROD reduced Dataset: {len(synROD_reduced_dataset)}')\n",
        "\n",
        "indexes = reduce_set(ROD_dataset, size=VALIDATION_SIZE)\n",
        "ROD_validation_dataset = Subset(ROD_dataset, indexes)\n",
        "print(f'ROD validation Dataset: {VALIDATION_SIZE}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "synROD train Dataset: 37528\n",
            "ROD Dataset: 32476\n",
            "synROD reduced Dataset: 32476\n",
            "ROD validation Dataset: 6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2bhrnX57TuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "synROD_dataloader = DataLoader(synROD_reduced_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "ROD_dataloader = DataLoader(ROD_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "ROD_dataloader_valid = DataLoader(ROD_validation_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUrks9_NkYMW",
        "colab_type": "text"
      },
      "source": [
        "#### Net & Loss & Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhLc9xJ3jMp1",
        "colab_type": "code",
        "outputId": "2712ba8a-1165-4108-c388-9eb55e7433ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "7fd4f6064d5c4ab5a1c8b6b001767169",
            "6346547a6bc04bfd846f79d6836a960e",
            "eab0364d2c554e2289d61fa8673c02fb",
            "b2bcd6eac33e4e748d8cd323cbb87236",
            "19c5a75e7573427f9db7b3d3a48eb74a",
            "925699e651984b24a0d2f84a2f3fc318",
            "8e6a837d1f2e4d33829383a8e006f8f3",
            "b08f8da3e04449b1abd621ba0a35def2"
          ]
        }
      },
      "source": [
        "net = magicnet(pretrained=True, num_classes=NUM_CLASSES)\n",
        "# Define loss function\n",
        "criterion_maintask = nn.CrossEntropyLoss() \n",
        "criterion_pretext_ROD = nn.CrossEntropyLoss()\n",
        "criterion_pretext_synROD = nn.CrossEntropyLoss()\n",
        "\n",
        "parameters_to_optimize = net.parameters() \n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd4f6064d5c4ab5a1c8b6b001767169",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfXcFRQkcgy",
        "colab_type": "text"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krecsC99Jieb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = net.to(DEVICE) \n",
        "cudnn.benchmark \n",
        "b_myloss_main = []\n",
        "b_validation = []\n",
        "b_current_step = 0\n",
        "\n",
        "b_best_net = None\n",
        "best_val = 0\n",
        "effective_ep = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq781AXUwSmN",
        "colab_type": "text"
      },
      "source": [
        "####Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKY9TZcdkeZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d532cdb-d2da-4b61-bc23-c8bee9ae335c"
      },
      "source": [
        "########################################## BASELINE: Only source \n",
        "# Start iterating over the epochs\n",
        "synROD_train_dataset.set_pretext(False)\n",
        "ROD_dataset.set_pretext(False)\n",
        "ROD_dataset.set_validation(True)\n",
        "\n",
        "ask_for_money = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  one_day_baby = time.time()\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, LR))\n",
        "\n",
        "  # Iterate over synROD\n",
        "  for images, labels in synROD_dataloader:\n",
        "    #MAIN TASK: Source data S, only classification of synROD\n",
        "    label = labels.to(DEVICE)\n",
        "    net.train() # Sets module in training mode\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    #load RGB and Depth on the GPU\n",
        "    imageRGB = images[0].to(DEVICE)\n",
        "    imageD = images[1].to(DEVICE)\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(rgb_x=imageRGB, dp_x=imageD) #main_task=True\n",
        "    loss_main = criterion_maintask(outputs, label)\n",
        "    loss_main.backward()\n",
        "    b_myloss_main.append(loss_main.item())\n",
        "    \n",
        "    if b_current_step % LOG_FREQUENCY == 0:\n",
        "      print(f'Step {b_current_step}, Loss {loss_main.item()}')\n",
        "\n",
        "    optimizer.step()\n",
        "    b_current_step += 1\n",
        "  \n",
        "  valid_value = test_validate(net, ROD_dataloader_valid, VALIDATION_SIZE, DEVICE)\n",
        "  #save the network with higher accuracy on validation\n",
        "  if valid_value > best_val:\n",
        "    best_val = valid_value\n",
        "    b_best_net = copy.deepcopy(net)\n",
        "\n",
        "  b_validation.append(valid_value)\n",
        "\n",
        "  will_be_old_oh_baby = time.time()\n",
        "  getTimes(one_day_baby, will_be_old_oh_baby, 'one EPOCH')\n",
        "\n",
        "  if stop_trining_4(b_validation):\n",
        "    effective_ep = epoch + 1\n",
        "    print(f'Accuracy scores are degenerating for four consecuive epochs at epoch:{effective_ep}')\n",
        "    break \n",
        "\n",
        "and_get_advice = time.time()\n",
        "if effective_ep ==-1:\n",
        "  effective_ep = NUM_EPOCHS\n",
        "getTimes(ask_for_money, and_get_advice, f'{effective_ep} EPOCHS')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/30, LR = 0.0003\n",
            "Step 0, Loss 5.560177803039551\n",
            "Step 50, Loss 4.532794952392578\n",
            "Step 100, Loss 3.3121891021728516\n",
            "Step 150, Loss 3.2785561084747314\n",
            "Step 200, Loss 2.586897611618042\n",
            "Step 250, Loss 1.9410436153411865\n",
            "Step 300, Loss 1.8872867822647095\n",
            "Step 350, Loss 1.5943145751953125\n",
            "Step 400, Loss 1.4279111623764038\n",
            "Step 450, Loss 1.3701285123825073\n",
            "Step 500, Loss 1.4317809343338013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33<00:00,  3.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 32.88%\n",
            "Time taken for one EPOCH: 00:07:13\n",
            "Starting epoch 2/30, LR = 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 550, Loss 1.3032169342041016\n",
            "Step 600, Loss 1.0879782438278198\n",
            "Step 650, Loss 1.2543408870697021\n",
            "Step 700, Loss 1.048809289932251\n",
            "Step 750, Loss 0.9443085789680481\n",
            "Step 800, Loss 1.0107002258300781\n",
            "Step 850, Loss 0.765306830406189\n",
            "Step 900, Loss 0.8795602321624756\n",
            "Step 950, Loss 1.153108835220337\n",
            "Step 1000, Loss 0.91214519739151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33<00:00,  2.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 39.05%\n",
            "Time taken for one EPOCH: 00:07:15\n",
            "Starting epoch 3/30, LR = 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1050, Loss 0.8060081005096436\n",
            "Step 1100, Loss 0.7704076170921326\n",
            "Step 1150, Loss 0.8545163869857788\n",
            "Step 1200, Loss 0.9653388857841492\n",
            "Step 1250, Loss 0.756986141204834\n",
            "Step 1300, Loss 0.6666218042373657\n",
            "Step 1350, Loss 0.8222084045410156\n",
            "Step 1400, Loss 0.8135946989059448\n",
            "Step 1450, Loss 0.6604987382888794\n",
            "Step 1500, Loss 0.6796592473983765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33<00:00,  2.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 40.59%\n",
            "Time taken for one EPOCH: 00:07:17\n",
            "Starting epoch 4/30, LR = 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1550, Loss 0.6486808061599731\n",
            "Step 1600, Loss 0.6127318143844604\n",
            "Step 1650, Loss 0.6817196607589722\n",
            "Step 1700, Loss 0.4540034830570221\n",
            "Step 1750, Loss 0.47717565298080444\n",
            "Step 1800, Loss 0.8372042775154114\n",
            "Step 1850, Loss 0.6412640810012817\n",
            "Step 1900, Loss 0.780264675617218\n",
            "Step 1950, Loss 0.6403431296348572\n",
            "Step 2000, Loss 0.47661760449409485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33<00:00,  2.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 40.28%\n",
            "Time taken for one EPOCH: 00:07:18\n",
            "Starting epoch 5/30, LR = 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 2050, Loss 0.6241031289100647\n",
            "Step 2100, Loss 0.7409690618515015\n",
            "Step 2150, Loss 0.5703162550926208\n",
            "Step 2200, Loss 0.565371036529541\n",
            "Step 2250, Loss 0.5629213452339172\n",
            "Step 2300, Loss 0.597931981086731\n",
            "Step 2350, Loss 0.5951456427574158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HMJnrYT3Ewd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_losses([b_myloss_main], sampling_size=25, save_to_png=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQrTodypcope",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_best_net.save_state_on_file('magicnet_baseline.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26yluzuXTXI",
        "colab_type": "text"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRuPMc9YXS_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluate on the full ROD dataset\n",
        "b_tast_value = test_validate(b_best_net, ROD_dataloader, len(ROD_dataset), DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEOFp73VXYdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracies(b_validation, b_tast_value, save_to_png=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c39ASAwRdGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print same summary\n",
        "steps_per_epochs = int(len(synROD_reduced_dataset) / BATCH_SIZE)\n",
        "argmax_v = np.argmax(b_validation) + 1\n",
        "best_loss = b_myloss_main[steps_per_epochs*argmax_v]\n",
        "print(f'----------------------------- Data for x ----------------------------')\n",
        "print(f'|{\"Test Acc\":8}|{\"Best Validation\":15}|{\"Best Epoch\":10}|{\"Total Epochs\":12}|{\"Corresponding Loss\":18}|')\n",
        "print(f'|{b_tast_value:8.5f}|{b_validation[argmax_v]:15.5f}|{argmax_v:10}|{effective_ep:12}|{best_loss:18}|')\n",
        "print(f'----------------------------------.----------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}